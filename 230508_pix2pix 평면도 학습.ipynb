{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4cb764c",
   "metadata": {},
   "source": [
    "# 이미지 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a14dde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.39it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = 'D:/datasets/G_pix2pix_dataset_floorplans/orgin_images'\n",
    "d_path = 'D:/datasets/G_pix2pix_dataset_floorplans'\n",
    "\n",
    "# a: 원본 | b: 라벨\n",
    "os.makedirs('{}/pre_done/a'.format(d_path), exist_ok = True)\n",
    "os.makedirs('{}/pre_done/b'.format(d_path), exist_ok = True)\n",
    "\n",
    "folder_list = os.listdir(path)\n",
    "name_cnt = 0\n",
    "name_list_list, name_list = [], []\n",
    "for folder in tqdm(folder_list):\n",
    "    img_list = os.listdir('{}/{}'.format(path, folder))\n",
    "    pre_name = 'none'\n",
    "    try: \n",
    "        for img_name in img_list:\n",
    "            img_name_split = img_name.split('_')\n",
    "            if pre_name == 'none': pre_name = img_name_split[0]\n",
    "            if img_name_split[0] == pre_name:\n",
    "                name_list_list.append(img_name_split)\n",
    "                name_list.append(img_name)\n",
    "            else:\n",
    "                pre_name = img_name_split[0]\n",
    "                rooms_ok, wall_ok = False, False\n",
    "                for name in name_list_list:\n",
    "                    if len(name) > 1:\n",
    "                        if name[1] == 'wall.png': wall_ok = True\n",
    "                        if name[1] == 'rooms.png': rooms_ok = True\n",
    "                if rooms_ok == True and wall_ok == True:\n",
    "                    for _name_list, name in zip(name_list_list, name_list):\n",
    "                        if len(_name_list) > 1:\n",
    "                            if _name_list[1] == 'wall.png':\n",
    "                                from_path = '{}/{}/{}'.format(path, folder, name)\n",
    "                                to_path = '{}/pre_done/b/{}.png'.format(d_path, name_cnt)\n",
    "                                shutil.copy(from_path, to_path)\n",
    "                            if _name_list[1] == 'rooms.png':\n",
    "                                from_path = '{}/{}/{}'.format(path, folder, name)\n",
    "                                to_path = '{}/pre_done/a/{}.png'.format(d_path, name_cnt)\n",
    "                                shutil.copy(from_path, to_path)\n",
    "                name_cnt += 1\n",
    "                name_list_list, name_list = [], []\n",
    "    except:\n",
    "        name_cnt += 1\n",
    "        name_list_list, name_list = [], []\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1a924",
   "metadata": {},
   "source": [
    "# 전처리 시각화 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ef7bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "path = 'D:/datasets/G_pix2pix_dataset_floorplans/pre_done'\n",
    "img_name_list = os.listdir('{}/a'.format(path))\n",
    "for img_name in img_name_list:\n",
    "    img_a = cv2.imread('{}/a/{}'.format(path, img_name))\n",
    "    img_b = cv2.imread('{}/b/{}'.format(path, img_name))\n",
    "    \n",
    "    img_a = cv2.resize(img_a, (640, 640))\n",
    "    img_b = cv2.resize(img_b, (640, 640))\n",
    "    concat_img = cv2.hconcat([img_a, img_b])\n",
    "    cv2.imshow('test', concat_img)\n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2012011",
   "metadata": {},
   "source": [
    "# 데이터 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2121758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 865/865 [00:00<00:00, 1230.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# a: 원본 | b: 라벨\n",
    "os.makedirs('{}/train/a'.format(d_path), exist_ok = True)\n",
    "os.makedirs('{}/train/b'.format(d_path), exist_ok = True)\n",
    "os.makedirs('{}/test/a'.format(d_path), exist_ok = True)\n",
    "os.makedirs('{}/test/b'.format(d_path), exist_ok = True)\n",
    "\n",
    "ratio = [0.8, 0.1, 0.1]\n",
    "path = 'D:/datasets/G_pix2pix_dataset_floorplans/pre_done'\n",
    "img_name_list = os.listdir('{}/a'.format(path))\n",
    "cnt = 0\n",
    "for img_name in tqdm(img_name_list):\n",
    "    if cnt > len(img_name_list) * ratio[0]:\n",
    "        mode = 'test'\n",
    "    else:\n",
    "        mode = 'train'\n",
    "    # copy a\n",
    "    from_path = '{}/pre_done/a/{}'.format(d_path, img_name)\n",
    "    to_path = '{}/{}/a/{}'.format(d_path, mode, img_name)\n",
    "    shutil.copy(from_path, to_path)\n",
    "    # copy b\n",
    "    from_path = '{}/pre_done/b/{}'.format(d_path, img_name)\n",
    "    to_path = '{}/{}/b/{}'.format(d_path, mode, img_name)\n",
    "    shutil.copy(from_path, to_path)\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfea6b0",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ffc1e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error loading msvcp140-e78ebc24b6ffa690be9375aacad743a7.dll; 작업을 완료했습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m log10 \u001b[38;5;66;03m# For metric function\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mldl\\lib\\site-packages\\matplotlib\\__init__.py:130\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m is_pyinstaller \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(lib_path)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mwindll\u001b[38;5;241m.\u001b[39mkernel32\u001b[38;5;241m.\u001b[39mLoadLibraryExW(ctypes\u001b[38;5;241m.\u001b[39mc_wchar_p(lib_path), \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0x00000008\u001b[39m):\n\u001b[0;32m    127\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(lib, ctypes\u001b[38;5;241m.\u001b[39mFormatError()))\n\u001b[1;32m--> 130\u001b[0m \u001b[43m_delvewheel_init_patch_1_3_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _delvewheel_init_patch_1_3_3\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# end delvewheel patch\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mldl\\lib\\site-packages\\matplotlib\\__init__.py:127\u001b[0m, in \u001b[0;36m_delvewheel_init_patch_1_3_3\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m lib_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(libs_dir, lib))\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m is_pyinstaller \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(lib_path)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mwindll\u001b[38;5;241m.\u001b[39mkernel32\u001b[38;5;241m.\u001b[39mLoadLibraryExW(ctypes\u001b[38;5;241m.\u001b[39mc_wchar_p(lib_path), \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0x00000008\u001b[39m):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(lib, ctypes\u001b[38;5;241m.\u001b[39mFormatError()))\n",
      "\u001b[1;31mOSError\u001b[0m: Error loading msvcp140-e78ebc24b6ffa690be9375aacad743a7.dll; 작업을 완료했습니다."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log10 # For metric function\n",
    "\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load Dataset from ImageFolder\n",
    "class Dataset(data.Dataset): # torch기본 Dataset 상속받기\n",
    "    def __init__(self, image_dir, direction):\n",
    "        super(Dataset, self).__init__() # 초기화 상속\n",
    "        self.direction = direction # \n",
    "        self.a_path = os.path.join(image_dir, \"a\") # a는 건물 사진\n",
    "        self.b_path = os.path.join(image_dir, \"b\") # b는 Segmentation Mask\n",
    "        self.image_filenames = [x for x in os.listdir(self.a_path)] # a 폴더에 있는 파일 목록\n",
    "        self.transform = transforms.Compose([transforms.Resize((256, 256)), # 이미지 크기 조정\n",
    "                                            transforms.ToTensor(), # Numpy -> Tensor\n",
    "                                             transforms.Normalize(mean=(0.5, 0.5, 0.5), \n",
    "                                                std=(0.5, 0.5, 0.5)) # Normalization : -1 ~ 1 range\n",
    "                                            ])\n",
    "        self.len = len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # 건물사진과 Segmentation mask를 각각 a,b 폴더에서 불러오기\n",
    "        a = Image.open(os.path.join(self.a_path, self.image_filenames[index])).convert('RGB') # 건물 사진\n",
    "        b = Image.open(os.path.join(self.b_path, self.image_filenames[index])).convert('RGB') # Segmentation 사진\n",
    "        \n",
    "        # 이미지 전처리\n",
    "        a = self.transform(a)\n",
    "        b = self.transform(b)\n",
    "        \n",
    "        if self.direction == \"a2b\": # 건물 -> Segmentation\n",
    "            return a, b\n",
    "        else:  # Segmentation -> 건물\n",
    "            return b, a\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "train_dataset = Dataset(\"D:/datasets/G_pix2pix_dataset_floorplans/train/\", \"b2a\")\n",
    "test_dataset = Dataset(\"D:/datasets/G_pix2pix_dataset_floorplans/test/\", \"b2a\")\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, num_workers=0, batch_size=1, shuffle=True) # Shuffle\n",
    "test_loader = DataLoader(dataset=test_dataset, num_workers=0, batch_size=1, shuffle=False)\n",
    "\n",
    "# -1 ~ 1사이의 값을 0~1사이로 만들어준다\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "# 이미지 시각화 함수\n",
    "def show_images(real_a, real_b, fake_b):\n",
    "    plt.figure(figsize=(15,45))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(real_a.cpu().data.numpy().transpose(1,2,0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.imshow(real_b.cpu().data.numpy().transpose(1,2,0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.imshow(fake_b.cpu().data.numpy().transpose(1,2,0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "# Conv -> Batchnorm -> Activate function Layer\n",
    "'''\n",
    "코드 단순화를 위한 convolution block 생성을 위한 함수\n",
    "Encoder에서 사용될 예정\n",
    "'''\n",
    "def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True, activation='relu'):\n",
    "    layers = []\n",
    "    \n",
    "    # Conv layer\n",
    "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    \n",
    "    # Batch Normalization\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    \n",
    "    # Activation\n",
    "    if activation == 'lrelu':\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "    elif activation == 'relu':\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == 'tanh':\n",
    "        layers.append(nn.Tanh())\n",
    "    elif activation == 'none':\n",
    "        pass\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Deconv -> BatchNorm -> Activate function Layer\n",
    "'''\n",
    "코드 단순화를 위한 convolution block 생성을 위한 함수\n",
    "Decoder에서 이미지 복원을 위해 사용될 예정\n",
    "'''\n",
    "def deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True, activation='lrelu'):\n",
    "    layers = []\n",
    "    \n",
    "    # Deconv.\n",
    "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    \n",
    "    # Batchnorm\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    \n",
    "    # Activation\n",
    "    if activation == 'lrelu':\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "    elif activation == 'relu':\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == 'tanh':\n",
    "        layers.append(nn.Tanh())\n",
    "    elif activation == 'none':\n",
    "        pass\n",
    "                \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Unet encoder\n",
    "        self.conv1 = conv(3, 64, 4, bn=False, activation='lrelu') # (B, 64, 128, 128)\n",
    "        self.conv2 = conv(64, 128, 4, activation='lrelu') # (B, 128, 64, 64)\n",
    "        self.conv3 = conv(128, 256, 4, activation='lrelu') # (B, 256, 32, 32)\n",
    "        self.conv4 = conv(256, 512, 4, activation='lrelu') # (B, 512, 16, 16)\n",
    "        self.conv5 = conv(512, 512, 4, activation='lrelu') # (B, 512, 8, 8)\n",
    "        self.conv6 = conv(512, 512, 4, activation='lrelu') # (B, 512, 4, 4)\n",
    "        self.conv7 = conv(512, 512, 4, activation='lrelu') # (B, 512, 2, 2)\n",
    "        self.conv8 = conv(512, 512, 4, bn=False, activation='relu') # (B, 512, 1, 1)\n",
    "\n",
    "        # Unet decoder\n",
    "        self.deconv1 = deconv(512, 512, 4, activation='relu') # (B, 512, 2, 2)\n",
    "        self.deconv2 = deconv(1024, 512, 4, activation='relu') # (B, 512, 4, 4)\n",
    "        self.deconv3 = deconv(1024, 512, 4, activation='relu') # (B, 512, 8, 8) # Hint : U-Net에서는 Encoder에서 넘어온 Feature를 Concat합니다! (Channel이 2배)\n",
    "        self.deconv4 = deconv(1024, 512, 4, activation='relu') # (B, 512, 16, 16)\n",
    "        self.deconv5 = deconv(1024, 256, 4, activation='relu') # (B, 256, 32, 32)\n",
    "        self.deconv6 = deconv(512, 128, 4, activation='relu') # (B, 128, 64, 64)\n",
    "        self.deconv7 = deconv(256, 64, 4, activation='relu') # (B, 64, 128, 128)\n",
    "        self.deconv8 = deconv(128, 3, 4, activation='tanh') # (B, 3, 256, 256)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        # Unet encoder\n",
    "        e1 = self.conv1(input)\n",
    "        e2 = self.conv2(e1)\n",
    "        e3 = self.conv3(e2)\n",
    "        e4 = self.conv4(e3)\n",
    "        e5 = self.conv5(e4)\n",
    "        e6 = self.conv6(e5)\n",
    "        e7 = self.conv7(e6)\n",
    "        e8 = self.conv8(e7)\n",
    "                              \n",
    "        # Unet decoder\n",
    "        d1 = F.dropout(self.deconv1(e8), 0.5, training=True)\n",
    "        d2 = F.dropout(self.deconv2(torch.cat([d1, e7], 1)), 0.5, training=True)\n",
    "        d3 = F.dropout(self.deconv3(torch.cat([d2, e6], 1)), 0.5, training=True)\n",
    "        d4 = self.deconv4(torch.cat([d3, e5], 1))\n",
    "        d5 = self.deconv5(torch.cat([d4, e4], 1))\n",
    "        d6 = self.deconv6(torch.cat([d5, e3], 1))\n",
    "        d7 = self.deconv7(torch.cat([d6, e2], 1))\n",
    "        output = self.deconv8(torch.cat([d7, e1], 1))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = conv(6, 64, 4, bn=False, activation='lrelu')\n",
    "        self.conv2 = conv(64, 128, 4, activation='lrelu')\n",
    "        self.conv3 = conv(128, 256, 4, activation='lrelu')\n",
    "        self.conv4 = conv(256, 512, 4, 1, 1, activation='lrelu')\n",
    "        self.conv5 = conv(512, 1, 4, 1, 1, activation='none')\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        out = self.conv1(input)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.conv5(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922321f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator와 Discriminator를 GPU로 보내기\n",
    "G = Generator().cuda()\n",
    "D = Discriminator().cuda()\n",
    "\n",
    "G.train()\n",
    "D.train()\n",
    "\n",
    "criterionL1 = nn.L1Loss().cuda()\n",
    "criterionMSE = nn.MSELoss().cuda()\n",
    "\n",
    "# Setup optimizer\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "epoch_set = 100\n",
    "\n",
    "# Train\n",
    "for epoch in range(1, epoch_set):\n",
    "    for i, (real_a, real_b) in enumerate(train_loader, 1):\n",
    "        # forward\n",
    "        real_a, real_b = real_a.cuda(), real_b.cuda()\n",
    "        real_label = torch.ones(1).cuda()\n",
    "        fake_label = torch.zeros(1).cuda()\n",
    "        \n",
    "        fake_b = G(real_a) # G가 생성한 fake Segmentation mask\n",
    "        \n",
    "        #============= Train the discriminator =============#\n",
    "        # train with fake\n",
    "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
    "        pred_fake = D.forward(fake_ab.detach())\n",
    "        loss_d_fake = criterionMSE(pred_fake, fake_label)\n",
    "\n",
    "        # train with real\n",
    "        real_ab = torch.cat((real_a, real_b), 1)\n",
    "        pred_real = D.forward(real_ab)\n",
    "        loss_d_real = criterionMSE(pred_real, real_label)\n",
    "        \n",
    "        # Combined D loss\n",
    "        loss_d = (loss_d_fake + loss_d_real) * 0.5\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        D.zero_grad()\n",
    "        loss_d.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        #=============== Train the generator ===============#\n",
    "        # First, G(A) should fake the discriminator\n",
    "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
    "        pred_fake = D.forward(fake_ab)\n",
    "        loss_g_gan = criterionMSE(pred_fake, real_label)\n",
    "\n",
    "        # Second, G(A) = B\n",
    "        loss_g_l1 = criterionL1(fake_b, real_b) * 10\n",
    "        \n",
    "        loss_g = loss_g_gan + loss_g_l1\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        G.zero_grad()\n",
    "        D.zero_grad()\n",
    "        loss_g.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print('======================================================================================================')\n",
    "            print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, g_loss: %.4f'\n",
    "                  % (epoch, epoch_set, i, len(train_loader), loss_d.item(), loss_g.item()))\n",
    "            print('======================================================================================================')\n",
    "            show_images(denorm(real_a.squeeze()), denorm(real_b.squeeze()), denorm(fake_b.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c475232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 저장\n",
    "path2models = 'D:/datasets/G_pix2pix_dataset_floorplans/models/'\n",
    "os.makedirs(path2models, exist_ok=True)\n",
    "path2weights_gen = os.path.join(path2models, 'weights_gen.pt')\n",
    "path2weights_dis = os.path.join(path2models, 'weights_dis.pt')\n",
    "\n",
    "torch.save(G.state_dict(), path2weights_gen)\n",
    "torch.save(D.state_dict(), path2weights_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a99114a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 불러오기\n",
    "weights = torch.load(path2weights_gen)\n",
    "G.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ceaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.eval()\n",
    "\n",
    "for i, (real_a, real_b) in enumerate(test_loader, 1):\n",
    "    # forward\n",
    "    real_a, real_b = real_a.cuda(), real_b.cuda()\n",
    "    real_label = torch.ones(1).cuda()\n",
    "    fake_label = torch.zeros(1).cuda()\n",
    "\n",
    "    fake_b = G(real_a) # G가 생성한 fake Segmentation mask\n",
    "    show_images(denorm(real_a.squeeze()), denorm(real_b.squeeze()), denorm(fake_b.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51e936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9483c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "mldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
